{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 機械学習によるテキスト分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "import mojimoji\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_tsv_path):\n",
    "    df = pandas.read_table(dataset_tsv_path, names=(\"TEXT\", \"LABEL\"))\n",
    "    return df['TEXT'].values, df['LABEL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset('./data/train.tsv')\n",
    "x_test, y_test = load_dataset('./data/test.tsv')\n",
    "x_train_val, x_test_val, y_train_val, y_test_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    #どっちでも\n",
    "    text = re.sub(',', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        #if (p == \".\"):\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析（Bag of Words・TF-IDFを使う場合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\"\"\"\n",
    "    *初期\n",
    "    mecabrc:(デフォルト)\n",
    "    -Ochasen:(ChaSen 互換形式)\n",
    "    -Owakati:(分かち書きのみを出力)\n",
    "    -Oyomi:(読みのみを出力)\n",
    "\n",
    "    *自分の環境の辞書も使える\n",
    "    -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd:neologd辞書\n",
    "    \"\"\"\n",
    "def mecab_tokenize(text):\n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    #tagger = MeCab.Tagger (\"-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    #node = tagger.parse(text)\n",
    "    #print(node.split(' '))\n",
    "    return tagger.parse(text).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(tokenizer=mecab_tokenize, preprocessor=preprocessing_text)\n",
    "\n",
    "x_train_val_vec = vectorizer.fit_transform(x_train_val)\n",
    "x_test_val_vec = vectorizer.transform(x_test_val)\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(tokenizer=mecab_tokenize, preprocessor=preprocessing_text)\n",
    "\n",
    "x_train_val_vec = vectorizer.fit_transform(x_train_val)\n",
    "x_test_val_vec = vectorizer.transform(x_test_val)\n",
    "\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTの単語の分散表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "bert_model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "def bert_vectorize_single(text):\n",
    "        max_length = 128\n",
    "        encoding = bert_tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        output = bert_model(**encoding)\n",
    "        last_hidden_state = output.last_hidden_state \n",
    "        return last_hidden_state[0][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_val_vec = [ bert_vectorize_single(preprocessing_text(x)) for x in x_train_val]\n",
    "x_test_val_vec = [ bert_vectorize_single(preprocessing_text(x)) for x in x_test_val ]\n",
    "\n",
    "x_train_vec = [ bert_vectorize_single(preprocessing_text(x)) for x in x_train]\n",
    "x_test_vec = [ bert_vectorize_single(preprocessing_text(x)) for x in x_test ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開発データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import count_nonzero\n",
    "import pickle\n",
    "\n",
    "DUMP_DIRNAME = './data/model'\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "#model = LinearSVC()\n",
    "\n",
    "model.fit(x_train_val_vec, y_train_val)\n",
    "\n",
    "y_pred=model.predict(x_test_val_vec)\n",
    "\n",
    "print(classification_report(y_test_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ハイパーパラメータの決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#SVM\n",
    "\"\"\"\n",
    "tuned_parameters = [\n",
    "    {'kernel': ['linear','rbf'],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,100.0,1000.0], \n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,100.0,1000.0]}\n",
    "    ]\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='f1' ) \n",
    "\"\"\"\n",
    "#LogisticRegression\n",
    "tuned_parameters = [\n",
    "    {'solverl': ['liblinear'],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,100.0,1000.0]}\n",
    "    ]\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=5, scoring='f1' ) \n",
    "\n",
    "clf.fit(x_train_val_vec, y_train_val)\n",
    "\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import count_nonzero\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from os import makedirs, path\n",
    "\n",
    "DUMP_DIRNAME = './data/model'\n",
    "\n",
    "model = LogisticRegression(C = 1, solver='liblinear')\n",
    "#model = LinearSVC( )\n",
    "#model = SVC(C=1.0, gamma=0.001, kernel='rbf')\n",
    "\n",
    "model.fit(x_train_vec, y_train)\n",
    "\n",
    "y_pred=model.predict(x_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1]\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "display(pandas.DataFrame(cm,\n",
    "    columns=[[\"Predicted\"] * len(labels), labels],\n",
    "    index=[[\"Actual\"] * len(labels), labels])\n",
    ")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC・AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "#plt.plot(fpr, tpr,marker=\".\")\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid()\n",
    "#plt.savefig(path_result+\"roc_curve.png\")\n",
    "auc=roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC:{}\".format(auc))\n",
    "\n",
    "\"\"\"\n",
    "with open(\"{}auc_f.txt\".format(path_result),\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"roc_curve, AUC:{}\\n\".format(auc))\n",
    "    f.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "#plt.plot(fpr, tpr,marker=\".\")\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid()\n",
    "#plt.savefig(path_result+\"precision_recall.png\")\n",
    "pr_auc=auc(recall, precision)\n",
    "print(\"AUC:{}\".format(pr_auc))\n",
    "\"\"\"\n",
    "with open(\"{}auc_f.txt\".format(path_result),\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"precision_recall, AUC:{}\\n\".format(pr_auc))\n",
    "    f.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}\".format((accuracy_score(y_test, y_pred))))\n",
    "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}\".format((precision_score(y_test, y_pred))))\n",
    "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}\".format((recall_score(y_test, y_pred))))\n",
    "print(\"F1（適合率と再現率の調和平均）={}\".format((f1_score(y_test, y_pred))))\n",
    "\n",
    "\"\"\"\n",
    "with open(\"{}auc_f.txt\".format(path_result),\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}\".format((accuracy_score(y_test, y_pred))))\n",
    "    f.write(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}\".format((precision_score(y_test, y_pred))))\n",
    "    f.write(\"再現率（positiveなデータに対してpositiveと予測された確率）={}\".format((recall_score(y_test, y_pred))))\n",
    "    f.write(\"F1（適合率と再現率の調和平均）={}\".format((f1_score(y_test, y_pred))))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import makedirs, path\n",
    "\n",
    "dirname = './data/model'\n",
    "filename = 'bert_model.pickle'\n",
    "makedirs(dirname, exist_ok=True)\n",
    "with open(path.join(dirname, filename), mode='wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "028baece2d5c9c450a926b56a2feb9be08d5da082e5e98dbd5d7f0ec42d420e8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
